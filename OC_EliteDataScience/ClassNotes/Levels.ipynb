{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro.\n",
    "- You'll better understand the algorithms you work with.\n",
    "- You'll anticipate more realistic timelines for your projects.\n",
    "- You'll spot low hanging fruit for model improvement.\n",
    "- You'll find it easier to stay motivated after poor initial results.\n",
    "- You'll be able to solve bigger problems with machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model Level: Fitting Parameters\n",
    "- ** Any model, whether it be a regression model, a decision tree, or a neural network, is defined by many (sometimes even millions) of model parameters.**\n",
    "    - For example, a regression model is defined by its feature coefficients, a decision tree is defined by its branch locations, and a neural network is defined by the weights connecting its layers.\n",
    "\n",
    "\n",
    "### Fitting Parameters with Gradient Descent\n",
    "- One of the shining successes in machine learning is the gradient descent algorithm (and its modified counterpart, stochastic gradient descent).\n",
    "- Gradient descent is an iterative method for finding the minimum of a function. In machine learning, that function is typically the loss (or cost) function. \"Loss\" is simply some metric that quantifies the cost of wrong predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Micro Level: Tuning Hyperparameters\n",
    "- Iteration plays a huge role is at what we named the \"micro\" level, more commonly known as the general model or model family.\n",
    "- You can think of a model family as broad category of models with customizable structures. Logistic regressions, decision trees, SVMs, and neural networks are actually all different families of models. \n",
    "    - Each model family has a set of structural choices you must make before actually fitting the model parameters.\n",
    "- **Hyperparameters**\n",
    "    - Within the logistic regression family, you can build separate models using either L1 or L2 regularization penalties. \n",
    "    - Within the decision tree family, you can have different models with different structural choices such as the depth of the tree, pruning thresholds, or even the splitting criteria.\n",
    "- **Hyperparameters** are \"higher-level\" parameters that cannot be learned directly from the data using gradient descent or other optimization algorithms. They describe structural information about a model that must be decided before fitting model parameters.\n",
    "- Tuning Hyperparameters with Cross-Validation.\n",
    "\n",
    "\n",
    "### Cross-validation \n",
    "- Is one of those techniques that works in so many scenarios that you'll almost feel like you're cheating when you use it.\n",
    "- Cross-validation is an iterative method for evaluating the performance of models built with a given set of hyperparameters.\n",
    "- With cross-validation, you can fit and evaluate models with various sets of hyperparameters using only your training data. That means you can save the test set as a true untainted hold-out set for your final model selection (more on this in the next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Macro Level: Solving Your Problem\n",
    "- **We're going to discuss iteration at the problem-solving level.**\n",
    "- ** Remember**: Fitting model parameters and tuning hyperparameters are only two parts of the entire machine learning problem-solving workflow.\n",
    "- Bc of the no free lunch theoreom, the best thing to do is...\n",
    "    - Therefore, one of the easiest ways to improve your solution for a given problem is to try several different model families. This level of iteration sits nicely above the previous level.\n",
    "    \n",
    "    \n",
    "### Ensembling Models\n",
    "- The next way to improve your solution is by combining multiple models into an ensemble. This is a direct extension from the iterative process needed to fit those models.\n",
    "- As a final word of caution: You should always keep an untainted test set to select your final model. We recommend splitting your data into train and test sets at the very beginning of your modeling process. Don't touch the test set until the very end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Meta Level: Improving Your Data\n",
    "- **Better data beats better algorithms.**\n",
    "- That doesn't always mean more data beats better algorithms. Yes, better data often implies more data, but it also implies cleaner data, more relevant data, and better features engineered from the data.\n",
    "\n",
    "\n",
    "### Collecting Better Data\n",
    "- The ability to collect better data is a skill that develops with time, experience, and more domain expertise. \n",
    "    - For example, if you're building a real estate pricing model, you need information on the house itself, the nearby neighborhood, and even past property tax payments that are publicly available.\n",
    "- The overall cleanliness of the data. \n",
    "    - That means having less missing data, lower measurement error, and doing your best to replace proxy metrics with primary metrics.\n",
    "    \n",
    "    \n",
    "### Engineering Better Features\n",
    "- Feature engineering, or creating new features from the data by leveraging domain knowledge, is one of the most valuable activities you can do to improve your models.\n",
    "- It's often difficult and time-consuming, but it's considered essential in applied machine learning. \n",
    "- That's because as you learn more about the domain, you'll develop better intuition around the types of features that are most impactful. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
