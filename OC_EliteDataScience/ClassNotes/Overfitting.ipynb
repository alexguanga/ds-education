{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal vs. Noise\n",
    "- In predictive modeling, you can think of the “signal” as the true underlying pattern that you wish to learn from the data.\n",
    "- “Noise,” on the other hand, refers to the irrelevant information or randomness in a dataset.\n",
    "- Noise interferes with signal.\n",
    "- A well functioning ML algorithm will separate the signal from the noise.\n",
    "- If the algorithm is too complex or flexible (e.g. it has too many input features or it’s not properly regularized), it can end up “memorizing the noise” instead of finding the signal.\n",
    "- This overfit model will then make predictions based on that noise. It will perform unusually well on its training data… yet very poorly on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit\n",
    "- In statistics, goodness of fit refers to how closely a model’s predicted values match the observed (true) values.\n",
    "- A model that has learned the noise instead of the signal is considered “overfit” because it fits the training dataset but has poor fit with new datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting vs. Underfitting\n",
    "- Both bias and variance are forms of prediction error in machine learning.\n",
    "- This trade-off between too simple (high bias) vs. too complex (high variance) is a key concept in statistics and machine learning, and one that affects all supervised learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Detect Overfitting\n",
    "- If our model does much better on the training set than on the test set, then we’re likely overfitting.\n",
    "- Another tip is to start with a very simple model to serve as a benchmark.\n",
    "- Then, as you try more complex algorithms, you’ll have a reference point to see if the additional complexity is worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Prevent Overfitting\n",
    "- **Cross-validation**\n",
    "    - Use your initial training data to generate multiple mini train-test splits. Use these splits to tune your model.\n",
    "    - Cross-validation allows you to tune hyperparameters with only your original training set. This allows you to keep your test set as a truly unseen dataset for selecting your final model.\n",
    "- **Train with more data**\n",
    "    - It won’t work everytime, but training with more data can help algorithms detect the signal better.\n",
    "- **Remove features**\n",
    "    - An interesting way to do so is to tell a story about how each feature fits into the model. This is like the data scientist's spin on software engineer’s rubber duck debugging technique, where they debug their code by explaining it, line-by-line, to a rubber duck.\n",
    "- **Early stopping**\n",
    "    - When you’re training a learning algorithm iteratively, you can measure how well each iteration of the model performs.\n",
    "    - Up until a certain number of iterations, new iterations improve the model. After that point, however, the model’s ability to generalize can weaken as it begins to overfit the training data.\n",
    "    - Early stopping refers stopping the training process before the learner passes that point.\n",
    "- **Regularization**\n",
    "    - The method will depend on the type of learner you’re using. For example, you could prune a decision tree, use dropout on a neural network, or add a penalty parameter to the cost function in regression.\n",
    "    - The regularization method is a hyperparameter as well, which means it can be tuned through cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
