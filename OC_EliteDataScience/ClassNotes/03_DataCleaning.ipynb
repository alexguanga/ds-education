{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unwanted observations\n",
    "- The first step to data cleaning is removing unwanted observations from your dataset.\n",
    "- This includes duplicate or irrelevant observations.\n",
    "     - Irrelevant observations are those that don’t actually fit the specific problem that you’re trying to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural errors\n",
    "- For instance, you can check for typos or inconsistent capitalization. This is mostly a concern for categorical features, and you can look at your bar plots to check.\n",
    "- Finally, check for mislabeled classes, i.e. separate classes that should really be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unwanted outliers\n",
    "- Outliers can cause problems with certain types of models. For example, linear regression models are less robust to outliers than decision tree models.\n",
    "- In general, if you have a legitimate reason to remove an outlier, it will help your model’s performance.\n",
    "- However, outliers are innocent until proven guilty. You should never remove an outlier just because it’s a \"big number.\" That big number could be very informative for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "- First, just to be clear, you cannot simply ignore missing values in your dataset. You must handle them in some way for the very practical reason that most algorithms do not accept missing values.\n",
    "- 2 most commonly recommended ways of dealing with missing data actually suck.\n",
    "    - Dropping observations that have missing values\n",
    "    - Imputing the missing values based on other observations\n",
    "- **DROPPING** the missing values can be bad bc the missing values may be informative and you are prob. going to have make predictions on data\n",
    "- **IMPUTING** the missing values is bad bc if you build a model to impute your values, you’re not adding any real information. You’re just reinforcing the patterns already provided by other features.\n",
    "\n",
    "### Dealing with missing values\n",
    "- ** Missing categorical data**\n",
    "    - The best way to handle missing data for categorical features is to simply label them as ’Missing’!\n",
    "    - You’re essentially adding a new class for the feature.\n",
    "    - This tells the algorithm that the value was missing.\n",
    "    - This also gets around the technical requirement for no missing values.\n",
    "- **Missing numeric data**\n",
    "    - For missing numeric data, you should flag and fill the values.\n",
    "    - Flag the observation with an indicator variable of missingness.\n",
    "    - Then, fill the original missing value with 0 just to meet the technical requirement of no missing values.\n",
    "    - By using this technique of flagging and filling, you are essentially allowing the algorithm to estimate the optimal constant for missingness, instead of just filling it in with the mean."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
