{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flaws of Linear Regression\n",
    "- The bad thing about linear regression is that they rarely work well in real-world application.\n",
    "- While they are easy to implement, and good for research articles, they aren't useful when we are trying to make predictions.\n",
    "- In this regard, simple linear regression suffers from two major flaws:\n",
    "    - It's prone to overfit with many input features.\n",
    "    - It cannot easily express non-linear relationships.\n",
    "\n",
    "### It's prone to overfit with many input features. Why does this happen?\n",
    "- Let's say you have 100 observations in your training dataset.\n",
    "- Let's say you also have 100 features.\n",
    "- If you fit a linear regression model with all of those 100 features, you can perfectly \"memorize\" the training set.\n",
    "- Each coefficient would simply memorize one observation. This model would have perfect accuracy on the training data, but perform poorly on unseen data.\n",
    "- It hasn’t learned the true underlying patterns; it has only memorized the noise in the training data.\n",
    "\n",
    "- **Regularization** is a technique used to prevent overfitting by artificially penalizing model coefficients.\n",
    "    - It can discourage large coefficients (by dampening them).\n",
    "    - It can also remove features entirely (by setting their coefficients to 0).\n",
    "    - The \"strength\" of the penalty is tunable. (More on this tomorrow...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## Regularized regression\n",
    "### Lasso Regression\n",
    "- Lasso, or LASSO, stands for Least Absolute Shrinkage and Selection Operator.\n",
    "- Lasso regression penalizes the absolute size of coefficients.\n",
    "- Practically, this leads to coefficients that can be exactly 0.\n",
    "- Thus, Lasso offers automatic feature selection because it can completely remove some features.\n",
    "- Remember, the \"strength\" of the penalty should be tuned.\n",
    "- A stronger penalty leads to more coefficients pushed to zero.\n",
    "\n",
    "### Ridge Regression\n",
    "- Ridge regression penalizes the squared size of coefficients.\n",
    "- Practically, this leads to smaller coefficients, but it doesn't force them to 0.\n",
    "- In other words, Ridge offers feature shrinkage.\n",
    "- Again, the \"strength\" of the penalty should be tuned.\n",
    "- A stronger penalty leads to coefficients pushed closer to zero.\n",
    "\n",
    "### Elastic-Net is a compromise between Lasso and Ridge.\n",
    "- Elastic-Net penalizes a mix of both absolute and squared size.\n",
    "- The ratio of the two penalty types should be tuned.\n",
    "- The overall strength should also be tuned.\n",
    "- Oh and in case you’re wondering, there’s no \"best\" type of penalty. It really depends on the dataset and the problem. We recommend trying different algorithms that use a range of penalty strengths as part of the tuning process, which we'll cover in detail tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "- Decision trees model data as a \"tree\" of hierarchical branches. They make branches until they reach \"leaves\" that represent predictions.\n",
    "- Due to their branching structure, decision trees can easily model nonlinear relationships.\n",
    "- For example, let's say that for Single Family homes, larger lots command higher prices.\n",
    "- However, let's that for Apartments, smaller lots command higher prices (i.e. it's a proxy for urban / rural).\n",
    "- This reversal of correlation is difficult for linear models to capture unless you explicitly add an interaction term (i.e. you can anticipate it ahead of time).\n",
    "- On the other hand, decision trees can capture this relationship naturally.\n",
    "- **Flaws**: \n",
    "    - Decisions trees suffer from a major flaw as well. If you allow them to grow limitlessly, they can completely \"memorize\" the training data, just from creating more and more and more branches.\n",
    "    - As a result, individual unconstrained decision trees are very prone to being overfit.\n",
    "- **Prevent Overfitting** \n",
    "- **1. ENSEMBLES** Ensembles are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the two most common are:\n",
    "    - **Bagging**: Bagging attempts to reduce the chance overfitting complex models.\n",
    "        - It trains a large number of \"strong\" learners in parallel.\n",
    "        - A strong learner is a model that's relatively unconstrained.\n",
    "        - Bagging then combines all the strong learners together in order to \"smooth out\" their predictions.\n",
    "    - **Boosting**: Boosting attempts to improve the predictive flexibility of simple models.\n",
    "        - It trains a large number of \"weak\" learners in sequence.\n",
    "        - A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree).\n",
    "        - Each one in the sequence focuses on learning from the mistakes of the one before it.\n",
    "        - Boosting then combines all the weak learners into a single strong learner.\n",
    "- While bagging and boosting are both ensemble methods, they approach the problem from opposite directions. Bagging uses complex base models and tries to \"smooth out\" their predictions, while boosting uses simple base models and tries to \"boost\" their aggregate complexity.\n",
    "- **Ensembling is a general term, but when the base models are decision trees, they have special names: random forests and boosted trees!**\n",
    "\n",
    "\n",
    "- **Random Forest**: train a large number of \"strong\" decision trees and combine their predictions through bagging.\n",
    "    - There are two sources of \"randomness\" for random forests:\n",
    "         - Each tree is only allowed to choose from a random subset of features to split on (leading to feature selection).\n",
    "         - Each tree is only trained on a random subset of observations (a process called resampling).\n",
    "    - Random forests tend to perform very well right out of the box.\n",
    "        - They often beat many other models that take up to weeks to develop.\n",
    "        - They are the perfect \"swiss-army-knife\" algorithm that almost always gets good results.\n",
    "        - They don’t have many complicated parameters to tune.\n",
    "        \n",
    "\n",
    "- **Boosted trees** train a sequence of \"weak\", constrained decision trees and combine their predictions through boosting.\n",
    "    - Each tree is allowed a maximum depth, which should be tuned.\n",
    "    - Each tree in the sequence tries to correct the prediction errors of the one before it.\n",
    "    - In practice, boosted trees tend to have the highest performance ceilings.\n",
    "    - They often beat many other types of models after proper tuning.\n",
    "    - They are more complicated to tune than random forests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
