{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator Variables\n",
    "- The first type of feature engineering involves using indicator variables to isolate key information.\n",
    "- \"Indicator variable\" I relate to indicator functions‡—so those can only be one or zero to indicate having or not having some property; therefore the term applies only to those used in reference-level coding\n",
    "- ** EXAMPLES**:\n",
    "    - **Indicator variable from thresholds**: Let's say you're studying alcohol preferences by U.S. consumers and your dataset has an age feature. You can create an indicator variable for age >= 21 to distinguish subjects who were over the legal drinking age.\n",
    "    - **Indicator variable from multiple features**: You're predicting real-estate prices and you have the features n_bedrooms and n_bathrooms. If houses with 2 beds and 2 baths command a premium as rental properties, you can create an indicator variable to flag them.\n",
    "    - **Indicator variable for groups of classes:** You're analyzing website conversions and your dataset has the categorical feature traffic_source. You could create an indicator variable for paid_traffic by flagging observations with traffic source values of  \"Facebook Ads\" or \"Google Adwords\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Features\n",
    "- **EXAMPLES**:\n",
    "    - **Sum of two features:** Let's say you wish to predict revenue based on preliminary sales data. You have the features sales_blue_pens and sales_black_pens. You could sum those features if you only care about overall sales_pens.\n",
    "    - **Difference between two features:** You have the features house_built_date and house_purchase_date. You can take their difference to create the feature house_age_at_purchase.\n",
    "    - **Product of two features:** You're running a pricing test, and you have the feature price and an indicator variable conversion. You can take their product to create the feature earnings.\n",
    "    - **Quotient of two features:** You have a dataset of marketing campaigns with the features n_clicks and n_impressions. You can divide clicks by impressions to create  click_through_rate, allowing you to compare across campaigns of different volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Representation\n",
    "- Your data won't always come in the ideal format. You should consider if you'd gain information by representing the same feature in a different way.\n",
    "- **EXAMPLES**:\n",
    "    - **Date and time features**: Let's say you have the feature purchase_datetime. It might be more useful to extract purchase_day_of_week and purchase_hour_of_day. You can also aggregate observations to create features such as purchases_over_last_30_days.\n",
    "    - **Numeric to categorical mappings**: You have the feature years_in_school. You might create a new feature grade with classes such as \"Elementary School\", \"Middle School\", and \"High School\".\n",
    "    - **Grouping sparse classes:** You have a feature with many classes that have low sample counts. You can try grouping similar classes and then grouping the remaining ones into a single \"Other\" class.\n",
    "    - **Creating dummy variables:** Depending on your machine learning implementation, you may need to manually transform categorical features into dummy variables. You should always do this after grouping sparse classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Data\n",
    "- An underused type of feature engineering is bringing in external data. This can lead to some of the biggest breakthroughs in performance.\n",
    "- For example, one way quantitative hedge funds perform research is by layering together different streams of financial data.\n",
    "- **EXAMPLES**:\n",
    "    - **Time series data**: The nice thing about time series data is that you only need one feature, some form of date, to layer in features from another dataset.\n",
    "    - **External API's**: There are plenty of API's that can help you create features. For example, the Microsoft Computer Vision API can return the number of faces from an image.\n",
    "    - **Geocoding**: Let's say have you street_address, city, and state. Well, you can geocode them into latitude and longitude. This will allow you to calculate features such as local demographics (e.g. median_income_within_2_miles) with the help of another dataset.\n",
    "    - **Other sources of the same data**: How many ways could you track a Facebook ad campaign? You might have Facebook's own tracking pixel, Google Analytics, and possibly another third-party software. Each source can provide information that the others don't track. Plus, any differences between the datasets could be informative (e.g. bot traffic that one source ignores while another source keeps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis (Post-Modeling)\n",
    "- The final type of feature engineering we'll cover falls under a process called error analysis. This is performed after training your first model.\n",
    "- Error analysis is a broad term that refers to analyzing the misclassified or high error observations from your model and deciding on your next steps for improvement.\n",
    "- Possible next steps include collecting more data, splitting the problem apart, or engineering new features that address the errors. To use error analysis for feature engineering, you'll need to understand why your model missed its mark.\n",
    "- **EXAMPLES**:\n",
    "    - **Start with larger errors:** Error analysis is typically a manual process. You won't have time to scrutinize every observation. We recommend starting with those that had higher error scores. Look for patterns that you can formalize into new features.\n",
    "    - **Segment by classes:** Another technique is to segment your observations and compare the average error within each segment. You can try creating indicator variables for the segments with the highest errors.\n",
    "    - **Unsupervised clustering:** If you have trouble spotting patterns, you can run an unsupervised clustering algorithm on the misclassified observations. We don't recommend blindly using those clusters as a new feature, but they can make it easier to spot patterns. Remember, the goal is to understand why observations were misclassified.\n",
    "    - **Ask colleagues or domain experts:** This is a great complement to any of the other three techniques. Asking a domain expert is especially useful if you've identified a pattern of poor performance (e.g. through segmentations) but don't yet understand why."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
