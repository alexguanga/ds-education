{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When the number of features is very large relative to the number of observations in your dataset, certain algorithms struggle to train effective models. This is called the **“Curse of Dimensionality,”** and it’s especially relevant for clustering algorithms that rely on distance calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "- Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones.\n",
    "- Some supervised algorithms already have built-in feature selection, such as Regularized Regression and Random Forests. \n",
    "\n",
    "\n",
    "- **Variance Thresholds**\n",
    "    - Variance thresholds remove features whose values don't change much from observation to observation (i.e. their variance falls below a threshold). These features provide little value.\n",
    "    - Example: In a public health dataset where 96% of observations for 35-year-old men, 'Age' and 'Gender' features can be eliminated without a major loss in information.\n",
    "    - Because variance is dependent on scale, you should always normalize your features first.\n",
    "    - **Strengths:** Applying variance thresholds is based on solid intuition: features that don't change much also don't add much information. This is an easy and relatively safe way to reduce dimensionality at the start of your modeling process.\n",
    "    - **Weaknesses:** If your problem does require dimensionality reduction, applying variance thresholds is rarely sufficient. Furthermore, you must manually set or tune a variance threshold, which could be tricky. We recommend starting with a conservative (i.e. lower) threshold.\n",
    "- **Correlation Thresholds**\n",
    "    - Correlation thresholds remove features that are highly correlated with others (i.e. its values change very similarly to another's). These features provide redundant information.\n",
    "    - Uou'd first calculate all pair-wise correlations. Then, if the correlation between a pair of features is above a given threshold, you'd remove the one that has larger mean absolute correlation with other features.\n",
    "    - **Strengths:** Applying correlation thresholds is also based on solid intuition: similar features provide redundant information. Some algorithms are not robust to correlated features, so removing them can boost performance.\n",
    "    - **Weaknesses:** You must manually set or tune a correlation threshold, which can be tricky to do. Plus, if you set your threshold too low, you risk dropping useful information. Whenever possible, we prefer algorithms with built-in feature selection over correlation thresholds. Even for algorithms without built-in feature selection, Principal Component Analysis (PCA) is often a better alternative.\n",
    "- **Genetic Algorithms (GA)**\n",
    "    - They search algorithms that are inspired by evolutionary biology and natural selection, combining mutation and cross-over to efficiently traverse large solution spaces.\n",
    "    - GA's have two main uses. \n",
    "        - The first is for optimization, such as finding the best weights for a neural network.\n",
    "        - The second is for supervised feature selection. In this use case, \"genes\" represent individual features and the \"organism\" represents a candidate set of features. Each organism in the \"population\" is graded on a fitness score such as model performance on a hold-out set. The fittest organisms survive and reproduce, repeating until the population converges on a solution some generations later.\n",
    "    - **Strengths:** Genetic algorithms can efficiently select features from very high dimensional datasets, where exhaustive search is unfeasible. When you need to preprocess data for an algorithm that doesn't have built-in feature selection (e.g. nearest neighbors) and when you must preserve the original features (i.e. no PCA allowed), GA's are likely your best bet. These situations can arise in business/client settings that require a transparent and interpretable solution.\n",
    "    - **Weaknesses:** GA's add a higher level of complexity to your implementation, and they aren't worth the hassle in most cases. If possible, it's faster and simpler to use PCA or to directly use an algorithm with built-in feature selection.\n",
    "- **Honorable Mention: Stepwise Search**\n",
    "    - Stepwise search is a supervised feature selection method based on sequential search, and it has two flavors: forward and backward. For forward stepwise search, you start without any features. Then, you'd train a 1-feature model using each of your candidate features and keep the version with the best performance. You'd continue adding features, one at a time, until your performance improvements stall.\n",
    "    - Backward stepwise search is the same process, just reversed: start with all features in your model and then remove one at a time until performance starts to drop substantially.\n",
    "    - THIS MODEL RARELY DOES WELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "- Feature extraction is for creating a new, smaller set of features that stills captures most of the useful information.\n",
    "\n",
    "\n",
    "- **Principal Component Analysis (PCA)**\n",
    "    - Principal component analysis is an unsupervised algorithm that creates linear combinations of the original features. The new features are orthogonal, which means that they are uncorrelated. They are ranked in order of their \"explained variance.\" The first principal component explains the most variance in your dataset, PC2 explains the second-most variance, and so on.\n",
    "    - **Strengths:** PCA is a versatile technique that works well in practice. It's fast and simple to implement, which means you can easily test algorithms with and without PCA to compare performance. In addition, PCA offers several variations and extensions (i.e. kernel PCA, sparse PCA, etc.) to tackle specific roadblocks.\n",
    "    - **Weaknesses:** The new principal components are not interpretable, which may be a deal-breaker in some settings. In addition, you must still manually set or tune a threshold for cumulative explained variance.\n",
    "- **Linear Discriminant Analysis (LDA)**\n",
    "    - LDA, not to be confused with latent Dirichlet allocation - also creates linear combinations of your original features. However, unlike PCA, LDA doesn't maximize explained variance. Instead, it maximizes the separability between classes.\n",
    "    - LDA is a supervised method that can only be used with labeled data.\n",
    "    - **Strengths:** LDA is supervised, which can (but doesn't always) improve the predictive performance of the extracted features. Furthermore, LDA offers variations (i.e. quadratic LDA) to tackle specific roadblocks.\n",
    "    - **Weaknesses:** As with PCA, the new features are not easily interpretable, and you must still manually set or tune the number of components to keep. LDA also requires labeled data, which makes it more situational.\n",
    "- **Autoencoders**\n",
    "    - Autoencoders are neural networks that are trained to reconstruct their original inputs. \n",
    "    - For example, image autoencoders are trained to reproduce the original images instead of classifying the image as a dog or a cat.\n",
    "    - The key is to structure the hidden layer to have fewer neurons than the input/output layers. \n",
    "    - **Strengths:** Autoencoders are neural networks, which means they perform well for certain types of data, such as image and audio data.\n",
    "    -**Weaknesses:** Autoencoders are neural networks, which means they require more data to train. They are not used as general-purpose dimensionality reduction algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
