{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression\n",
    "- Can have multiple features\n",
    "- Linear regression with multiple variables is also known as \"multivariate linear regression\".\n",
    "- m = the number of training examples\n",
    "- n = the number of feature\n",
    "- h$θ_{1}$=$θ_{0}$+$θ_{1}$$x_{1}$+$θ_{2}$$x_{2}$+$θ_{3}$$x_{3}$+⋯+$θ_{n}$$x_{n}$\n",
    "\n",
    "- ** Gradient Descent in Practice I **\n",
    "    - Theta are the parameters\n",
    "    - Feature Scaling: The values that the multiple parameters can take. If their range, scale, are similar, the algorithm can be conducted quickly. \n",
    "    - If you plot two parameters that differ in scaling, we can see in a coutour map that that the eclipse is quite narrow and will take a long time until it gets to the middle\n",
    "    - * Fix *: This can be fixed by scaling it or dividing it by the maximum possibility     \n",
    "    - You should try the feature into approximately from -1 to 1 (not required)\n",
    "    - Thumb of rule: Andrew likes ranges from -3 to 3, and -1/3 to 1/3\n",
    "    - **Mean normalization:** Replace xi with xi - mean to make features have approx. zero mean, and this is divided either by the number of ranges of numbers or the std. deviation\n",
    "    - This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\n",
    "    - Two techniques to help with this are feature scaling and mean normalization.   \n",
    "\n",
    "\n",
    "- **Gradient Descent in Practice II**\n",
    "    - Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. \n",
    "    - Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.\n",
    "    - *Debugging* - How to make that gradient descent is working correctly\n",
    "    - Kind of obvious, but J(θ) should decrease when increasing the number of iterations\n",
    "    - If you see that the gradient descent is increasing, then we probably need to decrease the learning rate\n",
    "    - For sufficient small alpha, J(θ) should decrease after iteration but don't choose the alpha to be **to small**\n",
    "    - ** NOTE **: Use different alphas, and check which alpha fits best!\n",
    "\n",
    "\n",
    "- **Features and Polynomial Regression**\n",
    "    - Creating new variables. Examples, instead of using frontage and depth, you could use the area \n",
    "    - You can fit better models by either squaring or cubicing the variables\n",
    "    - One thing to keep in mind is that the ranges might eventually become too large \n",
    "    - We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form).\n",
    "    \n",
    "    \n",
    "- ** NOTE**: Remember that x0 is 1 because when we create the matrix, we know that we are only talking about one item, say house, but the other features have different values because the increase at a different rate if it increases by one unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Parameters Algebrically\n",
    "- **Normal Equation**\n",
    "    - Equation: X * X(Transpose)^-1 * X(Transpose) * y\n",
    "    - Feature Scaling is NOT needed here, only if you're still performing gradient descent\n",
    "    - Gradient Descent vs Normal Equation? 10,000 seems to okay for normal matrix (10000 x 10000 matrix)\n",
    "    - **Gradient Descent**: Need to choose alpha, Needs many iterations, Works well when n is large\n",
    "    - **Normal Equation**: No need to choose alpha, No need to iterate, Slow if n is very large\n",
    "\n",
    "- Let’s discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In the \"Normal Equation\" method, we will minimize J by explicitly taking its derivatives with respect to the θj ’s, and setting them to zero. This allows us to find the optimum theta without iteration.\n",
    "    - θ = (X^T * X)^−1 * X^T * y\n",
    "    \n",
    "    \n",
    "- **Normal Equation Noninvertibility**\n",
    "    - When implementing the normal equation in octave we want to use the 'pinv' function rather than 'inv.' The 'pinv' function will give you a value of θ even if XTX is not invertible.\n",
    "    - If XTX is noninvertible, the common causes might be having :\n",
    "        - Redundant features, where two features are very closely related (i.e. they are linearly dependent) \n",
    "        - Too many features (e.g. m ≤ n). In this case, delete some features or use \"regularization\" (to be explained in a later lesson). \n",
    "    - Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
