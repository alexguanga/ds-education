{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descents with Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning With Large Datasets\n",
    "- For the most of the time, we want the largest datasets. This is great, and sometimes can outdo a better model, but there's a cost when we use large datasets.\n",
    "- The cost of using large datasets is computational power\n",
    "- If we have 1,000,000,000 features, this will take a lot of computational power. Why not just a subset of the examples of say 1,000?\n",
    "- One way to check if the larger dataset will perform better than subset is to ** plot a learning curve of range of values of m and verify that the algorithm has high variance when m is small.**\n",
    "- We need a high variance bc a higher variance means that there's a lower bias, and we are more closer to the actual!\n",
    "<img src=\"../images/LargeDatasets-1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "** Linear regression with Gradient Descent**\n",
    "- The problem with using gradient descent where m is large, we will have a hard time in computational power\n",
    "<img src=\"LargeDatasets-2.png\" alt=\"Drawing\" style=\"width: 300px;\"/>  <img src=\"LargeDatasets-3.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "- **Stochastic Gradient Descent**\n",
    "    - We have to randomly shuffle the dataset\n",
    "    - The difference here is that unlike the batch gradient descent, where it focuses on all the summation of the data to find the thetas...\n",
    "    - We only focus on one observations, and find the best theta for the observations which will continue to do it for all the features.\n",
    "    - But using the Stochastic uses much more and it doesn't always reach the global minimum\n",
    "    - But when you do for loop, you can do it between 1 to 10 times\n",
    "    - If m is very large, we don't have to do reiterate a lot\n",
    "<img src=\"../images/LargeDatasets-4.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "- The PATH USING A BATCH GRADIENT DESCENT IS THE FOLLOWING...\n",
    "<img src=\"../images/LargeDatasets-5.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "- The PATH USING A STOCHASTIC GRADIENT DESCENT IS THE FOLLOWING...\n",
    "<img src=\"../images/LargeDatasets-6.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "- Instead of using m, we will b\n",
    "- Where b is a mini batch size or the number of examples in each iteration\n",
    "- A typical b value would be around 10\n",
    "- And instead of doing the stochastic descent over one feature, we will do to b values\n",
    "<img src=\"../images/LargeDatasets-7.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "<img src=\"../images/LargeDatasets-8.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "- ** Why choose Mini-Batch Gradient Descent over Stochastic Gradient Descent?**\n",
    "    - It's bc of vectorization? (It must have a good vector implemenatation\n",
    "    - The sum of doing the 10 values, we can parallel find the solution of the 10 examples\n",
    "    - The B VALUE can be difficult in finding, but overall, this outperform the Stochastic Gradient Descent\n",
    "\n",
    "### Stochastic Gradient Descent Convergence\n",
    "- As stochastic gradient descent is scanning through our training set, right before we have updated the thetas, let's compute how well our hypothesis is doing on the training examples\n",
    "<img src=\"../images/LargeDatasets-9.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "- Sometimes a smaller learning parameter can be better\n",
    "- The larger the data oberservations, we can have a smoother graph\n",
    "<img src=\"../images/LargeDatasets-10.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "- One good way for the model to reach the minimum, it would be to decrease the learning parameter over time. A pretty typically way is using the following image\n",
    "<img src=\"../images/LargeDatasets-11.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online learning\n",
    "<img src=\"../images/AdvancedTopics-1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "- This focuses on the predicted CTR (Click-Through Rate)\n",
    "<img src=\"../images/AdvancedTopics-2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "### Map Reduce and Data Parallelism\n",
    "- You're pretty much dividing the data into n parts, where n is the number of computers or system you will be running in parallel, and compute each information in their computer\n",
    "- You'll then combine the results into a master server\n",
    "<img src=\"../images/AdvancedTopics-3.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "- If you plan on doing this, you must ask yourself if the results can be expressed as the summation of diff. results\n",
    "<img src=\"../images/AdvancedTopics-4.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "- You don't necessarily need 4 computers, but you can do is all in one computer using the four different cores so they can perform at the same time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
