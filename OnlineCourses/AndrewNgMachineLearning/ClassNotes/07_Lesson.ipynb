{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Margin Classification\n",
    "- Support Vector Machine (SPV)\n",
    "- This is very similar to the Logisic Regression\n",
    "- However, instead of looking at the curves, we are approximating the curves with two straight lines\n",
    "- Look at the graphs below\n",
    "<img src=\"../images/Logistic-Compare.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "<img src=\"../images/SPV-1.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "<img src=\"./images/SPV-2.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "\n",
    "\n",
    "** Large Margin Intuiton **\n",
    "- Unlike the logistic regression, where the treshold was content with 0, the SPV rewards you if these cost Theta^TransposeX is more than 1 or less than -1. \n",
    "- Then, the cost function would be 0.\n",
    "<img src=\"../images/SPV-3.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "<img src=\"../images/SPV-4.png\"alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n",
    "- When we look at the decision boundaries, the optimally you want to divide the classification is with the largest margin possible. Meaning, you want the information to be divided as far as possible from the obersevations.\n",
    "\n",
    "- Instead of lambda, the C is the regularization parameter\n",
    "\n",
    "\n",
    "** Mathematics Behind Large Margin Classification **\n",
    "- You have two vectors. If you look at the diagrams, we are sort of projecting one vector to another but the vector projeced is orthogonal\n",
    "<img src=\"../images/Math-SVM.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "<img src=\"../images/Math-SVM2.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "- We need to look at the classifier and see the projection of the observation onto it. If we pay attention to the inner products, we know that the graph follows the similar idea. Check the graph below to get a better feel.\n",
    "<img src=\"../images/SPV-6.png\" alt=\"Drawing\" style=\"width: 500px;\">\n",
    "<img src=\"../images/SPV-7.png\"alt=\"Drawing\" style=\"width: 500px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "- Gaussian Kernel\n",
    "- We are finding or redefining the new formulas instead of x. We are finding the similarities btw. x and some variable landmarks\n",
    "<img src=\"../images/Kernel.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "<img src=\"../images/Kernel-2.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "\n",
    "- If x is close to the landmark, then formula would be close to 0, but with the exp formula, the result is one\n",
    "- Check graph below\n",
    "\n",
    "- We manually pick the landmark points\n",
    "<img src=\"../images/Kernel-3.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    " \n",
    "- The $Sigma^{2}$ influences the function. Check below for graph of diff. $Sigma^{2}$\n",
    "<img src=\"../images/Kernel-4.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    " \n",
    "- We predict the values depedning on how similar they are too the landmarks.\n",
    "- I still don't know how the decide the landmarks. That should be solved!\n",
    "<img src=\"../images/Kernel-5.png\" alt=\"Drawing\" style=\"width: 700px;\">\n",
    "\n",
    "\n",
    "** SVM Parameters: **\n",
    "- Bc C = 1/lambda\n",
    "     - Large C ( or small lambda) means that we will have a lower bias but higher variance\n",
    "     - Small C ( or large lambda) means that we will have a higher bias but lower variance\n",
    "- $Sigma^{2}$\n",
    "    - A large $Sigma^{2}$ has a higher bias and low variance. Since, the graph run smoothly and most results will be similar.  \n",
    "    - A small $Sigma^{2}$ has a lower bias and higher variance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using an SVM\n",
    "- There are two options:\n",
    "    - Choice of parameters C\n",
    "    - Choice of Kernel\n",
    "        - Can be no kernel (or its called \"linear kernel\")\n",
    "        - Gaussian Kernel:\n",
    "            - If you choose this, then you have to decide $Sigma^{2}$\n",
    "            - Also, you do have to worry about normalization\n",
    "- When you do decide to use the kernel, they need to satisfy the Mercer's Theorem\n",
    "- Other kernels: Polynomial kernels, String kernel, Chi-square kernel, histogram intersection kernel\n",
    "\n",
    "** Mutli-class classification **\n",
    "- Very similar to the logistic regression\n",
    "\n",
    "** Logistic Regression Vs. SVMs **\n",
    "- If n is relative large to m (features to observations) then Andrew recommends using Logistic Regression os SVM without a kernel\n",
    "- If n is small and m is intermediate size, you should use SVM with a Gaussian kernel\n",
    "- If i small and m is large, create add/more features, then use logistic regression or SVM without a kernel\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
