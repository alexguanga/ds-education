{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks for the best line to seperate both these classes\n",
    "- The line is seearched through the maximum margin\n",
    "- The two observations that are closest to the maximum margin are called the support vectors\n",
    "- The line in the middle of the SVM is \"The Maximum Margin Hyperplane\" or Maximum Margin Classifier\n",
    "- This can be eithe or, but the two dotted line (in our 2D, it will be 2) is the negative hyperplane and the positive hyperplane. Meaning, anything in that class will be label 0, anything in the positive will be labeled 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's so special of the SVMs?\n",
    "- If you think about it as orange and apples\n",
    "- Most models, try to find the info of that batch, in this case apples and oranges and classifies it as their respective fruit\n",
    "- However, SVMs looks are the apples that are most like oranges and oranges that are most likely apples\n",
    "- Thus, it uses these 2 extreme observations as the support vector\n",
    "- Hence, it will then create the Maximum Margin Hyperplane "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels\n",
    "- Sometimes the data cannot be separated by the linear boundary\n",
    "- The whole point is to make a plane in whatever dimension it is, to a higher plane, create some equation, so it can then become a linear seperable in the hyperplane.\n",
    "- You then bring that method of classification into the original dimension\n",
    "- However, this requires a lot of computational power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Trick\n",
    "- If we look at the image, we see that the l represents a point in the lower dimensin (2-D)\n",
    "- The forumala then does some computation to put the image into a higher-level dimension\n",
    "- img \" /Users/Karen/Desktop/RBFKernel.png\"\n",
    "- In the RMF Kernel, you project the little hill type figure\n",
    "- The middle of that figure projected back to the 2D provides us with the kernel\n",
    "- We then use points and compare it to the formula using the to formula in the picture\n",
    "- You must realize that the larger the distance, the smaller the exponent because e^-(A large number) is small!\n",
    "- Thus, in the 3D figure, this image is not that high\n",
    "- On the other hand, if the distance is small, the number is smaller and thus, the exponent is larger!\n",
    "    - Thus, we have a number that is at a higher level\n",
    "- The role of sigma in this equation is how wide the circumference of the circle that's projected into the higher level should be\n",
    "- You are not doing the compuation in the higher plane, we are projecting an image that allows us to use the compuatuation in the normal dimension and not the n+1 dimension!!!\n",
    "- You can have two kernels and add them together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Kernel:\n",
    "- Gaussian RMF Kernel\n",
    "- Sigmoid Kernel\n",
    "    - Anything to the right will be YES\n",
    "    - Anything to the left will be NO\n",
    "- Polynomial Kernel\n",
    "- **ADD IMAGES**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
