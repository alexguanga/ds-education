{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Representation\n",
    "<img src=\"../images/Logistic-1.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "<img src=\"../images/Logistic-2.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "- Need a value between 0 and 1\n",
    "- Sigmoid/Logistic Regression\n",
    "- It crosses 0.5 on the y-intercept\n",
    "- $hθ_{x}$ will give us the probability that our output is 1. For example, $hθ_{x}$ =0.7 gives us a probability of 70% that our output is 1. Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).\n",
    "- We use the probability of 0.5 as the treshold (anything higher or equal to 0.5 is 1, anything lower than 0.5 is 0)\n",
    "- But when does the probability equal 0.5?\n",
    "- If we look at the graph, we notice that the probability of 0.5 occurs towards the right of the y-axis (x=0)\n",
    "- Use the slide for a better visual but if g($θ^{T}$ * x) is greater than 0, then the probability will be 1\n",
    "- Likewise, if the g($θ^{T}$ * x) is less than 0, then the probability will be 0\n",
    "- Can't use the same cost function that was use to find the linear function (because the function isn't linear anymore)\n",
    "\n",
    "### Decision boundary (line that separates the 0 and 1)\n",
    "- Nonlinear boundaries: Can have a circle as a decision boundary.\n",
    "- The decision boundary is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n",
    "\n",
    "- Check journal for more information on this\n",
    "- ** Cost Function **\n",
    "- When y=1\n",
    "    - We use the -log function because the cost is 0 (desired)  if y=1 and $hθ_{x}$=1 \n",
    "    - But when the function approaches 0, well the number could go until infinity\n",
    "    - There's a LARGE cost if $hθ_{x}$=0 (or as it approaches 0)\n",
    "    - The intuition says that the probability that y is equal to 1 is VERY small\n",
    "    - But if we are wrong, we'll penalize the learning algorithm by a large cost\n",
    "- When y=0\n",
    "    - Same intuition but the reverse\n",
    "    - Thus, if we see that the cost is 0 if y=0 and $hθ_{x}$=0\n",
    "\n",
    "\n",
    "- The gradient descent formular is the same as the linear regression\n",
    "- The thing that changes is how we compute the hypothesis. \n",
    "- For linear regression, we multiply theta^Transpose * X\n",
    "- For logistic regression, we divide 1 / 1 + e($-θ^{T}$* X)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification\n",
    "- one vs ALL \n",
    "- You compute one class vs the rest of the data through each individual class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Problem of Overfitting\n",
    "\n",
    "\n",
    "- **Overfitting**\n",
    "    - If a model underfits, say that it doesn't do a good job in prediciting the outcome, we call it \"High Bias\"\n",
    "    - At another extreme, we might have a model that predicts it quite accurate. The shortcoming is that it's overfitting the data\n",
    "    - We call this \"High Variance\"\n",
    "    - Overfitting: If we have too many features, the learned hypothesis may fit the training data set very well but fail to generalize to new examples \n",
    "    - Overfitting can be an issue if we  a lot of features bt enough training data\n",
    "\n",
    "\n",
    "- **Adressing Overfitting:**\n",
    "    - Options:\n",
    "        - Reduce the number of features\n",
    "        - Manually select which features to keep\n",
    "        - Model selection algorithm \n",
    "    - Regularization\n",
    "        - Keep all the features, but reduce magnitude/values parameters \n",
    "        - Works well when we have a lot of features, each of which contributes a bit to predicting y\n",
    "\n",
    "- ** NOTES**\n",
    "    - Underfitting, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features.\n",
    "    - Overfitting, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data.\n",
    "    \n",
    "    \n",
    "- ** Regularization**\n",
    "    - We can make the parameters, thetas, smaller.\n",
    "    - For examples, if we have 4 features and it might be overfitting, we can reduce the latter two parameters (where they are practically 0) and we are left with a quadratic function. Thus, we have a more general model, one that isn't overfitting as much.\n",
    "    - Lambda is the regularization parameter\n",
    "    - Lambda can't be too large because it will eliminate all the parameters, which will cause the model to UNDERFIT\n",
    "    - We've added two extra terms at the end to inflate the cost of θ3 and θ4. Now, in order for the cost function to get close to zero, we will have to reduce the values of θ3 and θ4 to near zero\n",
    "    - We don't want to penalize theta 0 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
