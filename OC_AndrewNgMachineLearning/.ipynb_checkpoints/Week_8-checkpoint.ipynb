{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "** Unsurpvised Learning: Introduction**\n",
    "- **Unsurpvised Learning**: We just ask the algorithm to find some structure in the dataset\n",
    "- Clustering algorithm finds some clusters, patterns, in the data set.\n",
    "- This is good for market segment, like in marketing, so you can sell things to different people differently.\n",
    "- Find coherrent group in a social network.\n",
    "- There are specific clustering algorithms\n",
    "\n",
    "\n",
    "** K-Means: **\n",
    "- First, choose two random points in the dataset. These points will be called **Cluster Centroids**\n",
    "- Then, each point gets assign to the specific classification is closet to.\n",
    "- It finds the average, and the original points changes from it original spot to the average point\n",
    "- There will be two things to keep in mind:\n",
    "    - K (number of clusters)\n",
    "    - Training sets\n",
    "    - <img src=\"Clustering-1.png\">\n",
    "- ** K-Means with no clustering**\n",
    "    - <img src=\"Clustering-2.png\">\n",
    "\n",
    "\n",
    "** Optimization Objective **\n",
    "- Remember: Capital K is the total number of clusters, lower k is the index of each individual cluster\n",
    "- c(i) is the index of the cluster to which the independent variable, x, is currently assigned too\n",
    "- <img src=\"cluster-3.png\">\n",
    "- The cost function is called **The distortion of the K-means Algorithm**\n",
    "- <img src=\"cluster-4.png\">\n",
    "\n",
    "\n",
    "** Random Initialization **\n",
    "- Should have K, clusters, less than the numbers of m, observations.\n",
    "- Just pick random points, and they will assign the random points. \n",
    "- The bad thing is that depending on the random points, you might get differen results\n",
    "- <img src=\"cluster-5.png\">\n",
    "- As we see, the bad thing is that the clusters can get stuck at the local optima where all the points are not correctly clustered\n",
    "- A solution can be to initialize K a lot of times\n",
    "- Repeating this whole process only works if our K isn't that large\n",
    "\n",
    "\n",
    "** Choosing the Number of Clusters **\n",
    "- The best way to choose the number of cluster is using visualization and picking it by hands\n",
    "- The right number of cluster doesn't have a clear-cut answers!\n",
    "- ** Elbow Method**:\n",
    "    - <img src=\"cluster-6.png\">\n",
    "    - The reason the elbow method is not use is because the cost function is usually like the right side where there's no clear-cut of where the \"elbow\" lies\n",
    "    - <img src=\"cluster-7.png\">\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "** Motivation l: Data Compression **\n",
    "- Reducing the amount of variables used in the equation\n",
    "    - From example, if we have inches and cms, maybe we can combine them as 1 single variable\n",
    "- If we have a 2-D point, we can project that onto a new 1-D line\n",
    "- <img src=\"motivation-1.png\">\n",
    "- Reducing 3-D into 2-D\n",
    "- Projecting the data from a 3-D onto a 2-D plane and then project into actual points in a 2-D graph\n",
    "- <img src=\"motivation-2.png\">\n",
    "- <img src=\"motivation-3.png\">\n",
    "\n",
    "\n",
    "** Motivation ll: Data Visualization **\n",
    "- We have a large dataset with many economic features, like GDP, Employment, etc\n",
    "- So we would like reduce this 50-D into a 2-D.\n",
    "- How do we plot or find these reduced features?\n",
    "- The two points or ideas we choose is GDP of economy, and GPD per capita\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "** Principal Component Analysis Problem Formulation **\n",
    "- We should find the points from the actual and the projection data are minimized or very closed to the projected line\n",
    "- ** A good practice is to do mean normalization before doing any calculation**\n",
    "- <img src=\"PCA-3.png\">\n",
    "- The same intution behind projecting it to a line is applied when projecting it into a plane\n",
    "- PCA IS **NOT** linear regression\n",
    "- PCA finds the way to minimize the magnitude of the line. \n",
    "- Linear regression finds the square difference between the predicted and actual point, they are vertical lines\n",
    "- <img src=\"PCA-4.png\">\n",
    "\n",
    "\n",
    "\n",
    "** Principal Component Analysis Algorithm **\n",
    "- Data Preprocessing\n",
    "    - Feature scaling / mean normilization\n",
    "- Reduce data from n-dimension to k-dimensions\n",
    "- ** Confusion Term**: Single Value Decomposition, eigenvectors\n",
    "- <img src=\"PCA-5.png\">\n",
    "- <img src=\"PCA-6.png\">\n",
    "- <img src=\"PCA-7.png\"> \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying PCA\n",
    "\n",
    "** Reconstruction from Compressed Representation **\n",
    "- Before we want from 2D to 1D or 3D to 2D, can we reverse the method and go from 1D to 2D. I don't know why would we do this\n",
    "- <img src=\"PCA-8.png\"> \n",
    "\n",
    "\n",
    "\n",
    "** Choosing the Number of Principal Components **\n",
    "- Average squared projection error\n",
    "- Total variation in the data: This tells you on average how far are the training examples from the origin\n",
    "- Don't worry that much on \"99% variance retained\", just know that this means that the formula below is less than one percent.\n",
    "- <img src=\"PCA-10.png\"> \n",
    "- There are two ways, look at the image below\n",
    "- For the right side, are dividing the red circle by the purple circle\n",
    "- <img src=\"PCA-11.png\"> \n",
    "\n",
    "\n",
    "** Advice for Applying PCA **\n",
    "- **Supervised learning speedup**\n",
    "    - If you're doing image processing, and you have a 100 by 100 image, you really have 10000 pixel feature and this can take a lot of time\n",
    "    - <img src=\"PCA-11.png\"> \n",
    "\n",
    "    - Thus, you need a better way to do\n",
    "    - ONLY PERFORM THE PCA ON THE TESTING SET AND NOT THE CV SET!\n",
    "- **Application of PCA**\n",
    "    - Compression\n",
    "        - Reduce memory/disk needed to store data\n",
    "        - Speed up the algorithm\n",
    "    - Visualization\n",
    "        - Works with k = 2 or k = 3\n",
    "- **Bad USE of PCA**\n",
    "    - People think that using reducing features for OVERFITTING\n",
    "    - You might throw some valuable information\n",
    "    - USE REGULARIZATION\n",
    "- <img src=\"PCA-12.png\"> \n",
    "\n",
    "    \n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
