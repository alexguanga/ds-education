{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a Learning Algorithm\n",
    "\n",
    "- It doesn't matter much that your parameters fit the dataset if it doesn't fit the training set.\n",
    "- The dataset is broken into three parts: training set, cross-validation set, and testing set.\n",
    "- An estimate of how the dataset are broken: 60%, 20%, 20%\n",
    "- We can now calculate three separate error values for the three different sets using the following method:\n",
    "    - Optimize the parameters in Θ using the training set for each polynomial degree.\n",
    "    - Find the polynomial degree d with the least error using the cross validation set.\n",
    "    - Estimate the generalization error using the test set with Jtest(Θ(d)), (d = theta from polynomial with lower error);\n",
    "\n",
    "\n",
    "** Additional Notes**\n",
    "- ** Training phase:** you present your data from your \"gold standard\" and train your model, by pairing the input with expected output.\n",
    "- ** Validation/Test phase:** in order to estimate how well your model has been trained (that is dependent upon the size of your data, the value you would like to predict, input etc) and to estimate model properties (mean error for numeric predictors, classification errors for classifiers, recall and precision for IR-models etc.)\n",
    "- **Application phase:** now you apply your freshly-developed model to the real-world data and get the results. Since you normally don't have any reference value in this type of data (otherwise, why would you need your model?), you can only speculate about the quality of your model output using the results of your validation phase.\n",
    "The validation phase is often split into two parts:\n",
    "    - In the first part you just look at your models and select the best performing approach using the validation data (=validation)\n",
    "    - Then you estimate the accuracy of the selected approach (=test).\n",
    "Hence the separation to 50/25/25.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Variance \n",
    "- Look at Graph\n",
    "- ** LEFT**: This is high bias problem. We are underfitting the model from the training to the cv or testing set. \n",
    "- ** RIGHT**: This is high variance problem. We are overfitting the model from the training to the cv or testing set. \n",
    "- ** BIAS**: The training and cv error will have high cost function\n",
    "- ** VARIANCE**: The training error will be low (you are fitting the training set well), and the cv error will be higher than the training set\n",
    "\n",
    "- ** Linear Regression with regularization **\n",
    "    - When there's a large lambda: We tend to underfit the data\n",
    "    - When there's a small lambda: We tend to overfit the data\n",
    "    - NEED TO LOOK FOR AN INTERMEDIATE LAMBDA\n",
    "    - LARGE LAMBDA: **HIGH BIAS** because we are underfitting (since we are changing the equation). Moreover, underfitting means that random event aren't taken into account. The visual is that we will have similar wrong prediction.\n",
    "    - SMALL LAMBDA: **HIGH VARIANCE** because we are close to zero (almost adding no value, so the equation remains the same) so there will be overfitting. Moreover, overfitting means that we can accuratly find different results centered around the correct prediction. \n",
    "<img src=\"Image7.png\">\n",
    "- ** Learning Curves **\n",
    "     - As you add more observation, the error increases because the model can fit better on small datasets rather than large datasets\n",
    "     - The more data, the more the cost function of the training and cross-validation are closer together\n",
    "     - ** If a learning algorithm is suffering from high bias**, getting more training data WILL NOT by itself help much\n",
    "     - This is because bias means that it suffers from underfitting. If create a simple model that doesn't consider all possible outcomes, we will constantly get the wrong result, hence, more data points won't do anything. \n",
    "<img src=\"Image8.png\">\n",
    "<img src=\"Image9.png\">\n",
    "         - There's a high error in the training set bc we are underfitting\n",
    "         - Look at the graphs above. When we have a simple model (since that's when underfitting occurs), adding more observation still causes underfitting. \n",
    "\n",
    "\n",
    "- ** If a learning algorithm is suffering from high variance**, getting more training data WILL by itself help much\n",
    "     - This occurs because variance means that it overfits our model. Variance means that we will differernt results. Because the model overfits the training sample, the model will get differnt results throughout each prediction. As we get more data, we will be able to find an average.\n",
    "<img src=\"Image10.png\">\n",
    "<img src=\"Image11.png\">\n",
    "         - The error continues to decrease because we can fit the data more as you begin to understand the data more\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras\n",
    "** If you have HIGH VARIANCE PROBLEM:**\n",
    "- You can get more training examples\n",
    "- Try smaller sets of features (bc you are overfitting)\n",
    "- Try increasing lambda, so you can not overfit the training set as much. The higher the lambda, the more the regularization applies. \n",
    "\n",
    "** If you have HIGH BIAS PROBLEM:**\n",
    "- Try getting additional features\n",
    "- Try adding polynomial features\n",
    "- Try decreasing lambda, so you can try to fit the data better. The lower the lambda, the less the regularization applies.\n",
    "\n",
    "**Model Complexity Effects:**\n",
    "- Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently.\n",
    "- Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance.\n",
    "- In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting in Machine Learning\n",
    "Overfitting refers to a model that models the training data too well.\n",
    "\n",
    "Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.\n",
    "\n",
    "### Underfitting in Machine Learning\n",
    "Underfitting refers to a model that can neither model the training data nor generalize to new data.\n",
    "\n",
    "An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND PART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spam Classifier\n",
    "- So how could you spend your time to improve the accuracy of this classifier?\n",
    "    - Collect lots of data (for example \"honeypot\" project but doesn't always work)\n",
    "    - Develop sophisticated features (for example: using email header data in spam emails)\n",
    "    - Develop algorithms to process your input in different ways (recognizing misspellings in spam).\n",
    "    - It is difficult to tell which of the options will be most helpful.\n",
    "    \n",
    "- Selecting the model or approach can be difficult bc there's no clearn approach on how to implement it. \n",
    "    - Thus, Andrew suggest making a simple model and then plotting a learning curve to see if you need more or less data\n",
    "    - Error analysis: Look at all the mistakes or error that the model did. Then, check the information and categorizes on a common ground of these errors. \n",
    "        - Stemming software: In the SPAM/NONSPAM example, it will find words that are the same and treat them the same. \n",
    "- It is very important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm's performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3% error rate instead of 5%, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2% error rate instead of 3%, then we should avoid using this new feature. Hence, we should try new things, get a numerical value for our error rate, and based on our result decide whether we want to keep the new feature or not.\n",
    "\n",
    "\n",
    "- **Error Analysis**\n",
    "     - The recommended approach to solving machine learning problems is to:\n",
    "     - Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.\n",
    "     - Plot learning curves to decide if more data, more features, etc. are likely to help.\n",
    "     - Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Skewed Data\n",
    "\n",
    "<img src=\"Confusion-Matrix.png\">\n",
    "\n",
    "- Just having an improvement on the classification isn't an optimally way to check the power of the model. For example, if there's an improvment from 99.2 to 99.5 accuracy, are we making better prediction or are we simply predicting 0 more times.\n",
    "- We create a Prediction/Actual box (in a binary problem, it would be a 2 by 2. \n",
    "\n",
    "- **Classifier Accuracy**\n",
    "    - (true positives + true negatives) / (total examples)\n",
    "    - This is a good way of measuring UNLESS the data is skewed into one direction\n",
    "    - If the data is skewed, we don't really know if the model is good or we simply are predicting the model to be more like the skewed data\n",
    "\n",
    "\n",
    "- Another method to use is **Recall/Precision**:\n",
    "    - **Precision**: Out of all the patient that we predicted that have cancer (or 1), what fraction actually have cancer?\n",
    "        - TRUE POSITIVES/PREDICTED POSITIVES -> TRUE POSITIVES/(TRUE POSITIVES + FALSE POSITIVES)\n",
    "        - Using the box, this is represented as row 1\n",
    "    - **Recall**: Out of all the patient that actually have cancer (or 1), what fraction did we correctly detect as having cancer?\n",
    "        - TRUE POSITIVES/ACTUAL POSITIVES -> TRUE POSITIVES/(TRUE POSITIVES + FALSE NEGATIVES)\n",
    "        - Using the box, this is represented as col. 1 \n",
    "\n",
    "#### Trading Off Precision and Recall\n",
    "- In cancer case, we can change the treshold of 0.5 to 0.7\n",
    "- If we do this, we will have a higher precision and a lower recall.\n",
    "- The tradeoff is the precision looks at the prediciton. How will did we predict. So even if we didn't predict ALL the values, if we did well on the predicitons we made, we have a good precision.\n",
    "- However, recall looks at all the actual values (that are 1) and see how well we predicted out of those. IT doesn't care if we, say predicted all the values are 1, bc it would return a high recall score since we predicted 1 to most of the actual 1's\n",
    "- It's NOT a good way to find the average of both scores!\n",
    "- A better way to evaluate these scores is to use a **F-SCORE or F1 Score**\n",
    "- **F-Score** falls btw 0 and 1, 0 being the worst and 1 being the best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Large Data Sets\n",
    "- Some researcher have used different models on different data sets. What they found is that the models that have the most data tend to the same. Thus, when all these models had more data to work on, they did roughly the same.\n",
    "- To have a low bias: We can make sure that our model has enough parameters to work with.\n",
    "- To have a low variance: We can make that our model has enough data (a lot)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
