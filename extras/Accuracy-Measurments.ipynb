{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ANOVA:**\n",
    "    - A statistical method used to compare the means of two or more groups. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** T-tests: **\n",
    "    - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** RSE:**\n",
    "    - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** RSS:**\n",
    "    - \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** R-Squared: **\n",
    "    - \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TSS:**\n",
    "    - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** F-Stats: **\n",
    "    - \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mallow’s Cp, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi-squared tests, permutation test, bootstrapping, jackknifing\n",
    "- SSR (Sum of Squared due to Regression):\n",
    "     - The difference between the regression and the mean\n",
    "- SSE (Sum of Squared due to Error):\n",
    "     - The difference between the actual observation and the expected value due to the regression\n",
    "- SST:\n",
    "    - The total sum (SSR + SSE)\n",
    "- R_Squared: Also called the coefficient of determination\n",
    "     - SSR/SST: Thus, you would a high R_Squared bc that means that line of best fit is close to the actual observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - To illustrate the basic idea of a permutation test, suppose we have two groups A and B whose sample means are bar_A and bar_B and that we want to test, at 5% significance level, whether they come from the same distribution. Let n_A and n_B be the sample size corresponding to each group. The permutation test is designed to determine whether the observed difference between the sample means is large enough to reject the null hypothesis that the two groups have identical probability distribution.\n",
    "     - The test proceeds as follows. First, the difference in means between the two samples is calculated: this is the observed value of the test statistic, T(obs). Then the observations of groups A and B are pooled.\n",
    "     - Next, the difference in sample means is calculated and recorded for every possible way of dividing these pooled values into two groups of size n_A and n_B (i.e., for every permutation of the group labels A and B). The set of these calculated differences is the exact distribution of possible differences under the null hypothesis that group label does not matter.\n",
    "     - The one-sided p-value of the test is calculated as the proportion of sampled permutations where the difference in means was greater than or equal to T(obs). The two-sided p-value of the test is calculated as the proportion of sampled permutations where the absolute difference was greater than or equal to ABS(T(obs)).\n",
    "     - If the only purpose of the test is reject or not reject the null hypothesis, we can as an alternative sort the recorded differences, and then observe if T(obs) is contained within the middle 95% of them. If it is not, we reject the hypothesis of identical probability curves at the 5% significance level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "- Multicollinearity generally occurs when there are high correlations between two or more predictor variables. In other words, one predictor variable can be used to predict the other. This creates redundant information, skewing the results in a regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Inflation Factor\n",
    "- A variance inflation factor(VIF) detects multicollinearity in regression analysis. Multicollinearity is when there’s correlation between predictors (i.e. independent variables) in a model; it’s presence can adversely affect your regression results. The VIF estimates how much the variance of a regression coefficient is inflated due to multicollinearity in the model.\n",
    "- Variance inflation factors range from 1 upwards. \n",
    "- For example, a VIF of 1.9 tells you that the variance of a particular coefficient is 90% bigger than what you would expect if there was no multicollinearity — if there was no correlation with other predictors.\n",
    "- 1 = not correlated.\n",
    "- Between 1 and 5 = moderately correlated.\n",
    "- Greater than 5 = highly correlated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
