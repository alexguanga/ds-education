{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDFs VS. PMFs\n",
    "- **PMFs**\n",
    " - This basically is a probability law for a continuous random variable say X (for discrete, it is probability mass function).\n",
    " - The probability law defines the chances of the random variable taking a particular value say x, i.e. P (X=x). \n",
    " - However this definition is not valid for continuous random variables because the probability at a given point is zero. \n",
    "- **CDFs**\n",
    " - As the name cumulative suggests, this is simply the probability upto a particular value of the random variable, say x. \n",
    " - Generally denoted by F, F= P (X<=x) for any value of x in the X space.  It is defined for both discrete and continuous random variables.\n",
    "- Where a distinction is made between probability function and density, the pmf applies only to discrete random variables, while the pdf applies to continuous random variables.\n",
    "- <img src=\"./images/Distributions.png\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions\n",
    "- ** Bernoulli:** The Bernoulli PDF has two lines of equal height, representing the two equally-probable outcomes of 0 and 1 at either end.\n",
    "- **Uniform:** Distribution over many equally-likely outcomes.\n",
    "- **Binomial:** The binomial distribution may be thought of as the sum of outcomes of things that follow a Bernoulli distribution. Its parameters are n, the number of trials, and p, the probability of a “success”. For the graph, think that the chances for most probabilities will be around the middle. Think of throwing a quarter and checking for success among three coins. For any extreme value(like all heads or tails), these will rarely happen and thus, must happen at the tails. \n",
    "- **Hypergeometric:** This is the distribution of that same count if the balls were drawn without replacement instead. It’s cousin to the binomial distribution because the probability of success changes as balls are removed. If the number of balls is large relative to the number of draws, the distributions are similar because the chance of success changes less with each draw. \n",
    "- **Poisson:** What about the count of customers calling a support hotline each minute? That’s an outcome whose distribution sounds binomial, if you think of each second as a Bernoulli trial in which a customer doesn’t call (0) or does (1). Let n go to infinity and let p go to 0 to match so that np stays the same. The Poisson distribution is what you must think of when trying to count events over a time given the continuous rate of events occurring. The Poisson distribution governs how many events happen in a given period of time\n",
    "    - <img src=\"./images/Dist_2.png\" style=\"width: 300px;\"/>\n",
    "    - Lambda: The average\n",
    "    - <img src=\"./images/Dist_3.png\" style=\"width: 300px;\"/>\n",
    "- **Geometric:** From simple Bernoulli trials arises another distribution. How many times does a flipped coin come up tails before it first comes up heads? Meaning, we want the total it fails before it's a success. Hence, we see the left-tail diminish\n",
    "- ** Negative Binomial:** A simple generalization. It’s the number of failures until r successes have occurred, not just 1. \n",
    "- ** Exponential:** It governs how much time elapses between consecutive events. Given events whose count per time follows a Poisson distribution, then the time between events follows an exponential distribution with the same rate parameter λ.\n",
    "- ** Weibull:** Whereas the exponential distribution is appropriate when the rate—of wear, or failure for instance—is constant, the Weibull distribution can model increasing (or decreasing) rates of failure over time. The exponential is merely a special case. Think of “Weibull” when the chat turns to time-to-failure.\n",
    "- ** Normal:** or Gaussian distribution, is maybe the most important of all. Its bell shape is instantly recognizable. Like e, it’s a curiously particular entity that turns up all over, from seemingly simple sources. Take a bunch of values following the same distribution—any distribution—and sum them. The distribution of their sum follows (approximately) the normal distribution. The more things that are summed, the more their sum’s distribution matches the normal distribution.\n",
    "- ** Log-Normal:** An outcome that follows a log-normal distribution takes on values whose logarithm is normally distributed. If sums of things are normally distributed, then remember that products of things are log-normally distributed. \n",
    "- ** Student’s t:** The basis of the t-test that many non-statisticians learn in other sciences. It’s used in reasoning about the mean of a normal distribution, and also approaches the normal distribution as its parameter increases. The distinguishing feature of the t-distribution are its tails, which are fatter than the normal distribution’s.\n",
    "- ** Chi-squared:** We square the normal distribution to get this distribution. The one parameter we can change is the degrees of freedom. The degrees of freedom is how much different variables we have in our formula. If the degrees of freedom is 1, we would expect that the chi-squared distribution is close to 0 the y-axis, since when using a normal distribution, our values are centered around 0. The distribution of the sum of squares of normally-distributed values.\n",
    "    - https://www.quora.com/What-is-the-most-intuitive-explanation-for-the-chi-square-test\n",
    "- ** Gamma ** It is a generalization of both the exponential and chi-squared distributions. More like the exponential distribution, it is used as a sophisticated model of waiting times. For example, the gamma distribution comes up when modeling the time until the next n events occur. It appears in machine learning as the “conjugate prior” to a couple distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exponential and Poisson distributions relate to the same kind of process (a Poisson process), but they govern different aspects: The Poisson distribution governs how many events happen in a given period of time, and the exponential distribution governs how much time elapses between consecutive events.\n",
    "- By way of analogy, suppose that we have a different process, in which events occur exactly every 10 seconds. Then the number of events that happen in a minute (i.e., 60 seconds) is deterministically 6, and the amount of time that elapses between consecutive events is, of course, deterministically 10 seconds.\n",
    "- In contrast, in a Poisson process with a mean rate of one event every 10 seconds (i.e., λ=1/10), the number of events that happen in a minute is not deterministically 6, but it has a mean of 6. The exact distribution is given by the Poisson distribution:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
