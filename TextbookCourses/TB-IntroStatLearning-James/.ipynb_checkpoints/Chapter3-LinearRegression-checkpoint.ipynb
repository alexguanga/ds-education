{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.1.3 Assessing the Accuracy of the Model**\n",
    "\n",
    "- **R2 Statistic**: The R2 statistic (3.17) has an interpretational advantage over the RSE\n",
    "(3.15), since unlike the RSE, it always lies between 0 and 1. However, it can\n",
    "still be challenging to determine what is a good R2 value, and in general,\n",
    "this will depend on the application.\n",
    "- The function can sometimes as a blackbox where we are not too concerned with the formuala, rather that the function provides some level of confidence. \n",
    "- There are two types of error:\n",
    "     - reducible error: You might be able to find new models that can provide a better accuracy to the specific prediction. This is possible with tweaks.\n",
    "     - irreducible error: This can never be reduced. We will always have information that has some errors with our models. \n",
    "     \n",
    "     \n",
    "<img src=\"Intro_Stats_1.png\"style=\"width: 400px;\"/>\n",
    "\n",
    "- Looking at the image, the left side is the improvements that model can achieve. We can find a betas to make the models more accurate! The left side, is the variance, and that's info. we cannot reduce. This can by the way we retrieve info. Things that we cannot control!\n",
    "\n",
    "- Two things to look when assesing the accuracy of the model: standard error (RSE) and the R-Squared statistic.\n",
    "- RSS: The error is represented by the difference the linear prediction and the actual observation.\n",
    "- What Is R-squared?: R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n",
    "- R-squared = Explained variation / Total variation, R-squared is always between 0 and 100%:\n",
    "- thus if we see a graph where there is a lot variation, or the data is not within the model, then this would have a low r-squared. Likewise, if the observations resembles the model, then we would have a high r-squared!\n",
    "- RSE: The RSE is an estimate of the standard. deviation of the error term. Roughly speaking, it is the average amount that the response will deviate from the true regression line. In the case of the advertising data, we see from the linear regression\n",
    "output in Table 3.2 that the RSE is 3.26. In other words, actual sales in\n",
    "each market deviate from the true regression line by approximately 3,260\n",
    "units, on average. Another way to think about this is that even if the\n",
    "model were correct and the true values of the unknown coefficients β0\n",
    "and β1 were known exactly, any prediction of sales on the basis of TV\n",
    "advertising would still be off by about 3,260 units on average. Of course,\n",
    "whether or not 3,260 units is an acceptable prediction error depends on the\n",
    "problem context. In the advertising data set, the mean value of sales over\n",
    "all markets is approximately 14,000 units, and so the percentage error is\n",
    "3,260/14,000 = 23%.\n",
    "- So the premise is this. WE have to find the how much we owul dbe away from the data, even if we all correct. This number has to depend on the population we are looking, check its relativety! **A smaller number here is BETTER**\n",
    "\n",
    "\n",
    "BIAS VS UNBIASED\n",
    "- Bias is the difference between the expected value of an estimator and the true value being estimated. For example the sample mean for a simple random sample (SRS) is an unbiased estimator of the population mean because if you take all the possible SRS's find their means, and take the mean of those means then you will get the population mean (for finite populations this is just algebra to show this). But if we use a sampling mechanism that is somehow related to the value then the mean can become biased, think of a random digit dialing sample asking a question about income. If there is positive correlation between number of phone numbers someone has and their income (poor people only have a few phone numbers that they can be reached at while richer people have more) then the sample will be more likely to include more people with higher incomes and therefore the mean income in the sample will tend to be higher than the population income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.2.2 Some Important Questions** \n",
    "- When we perform multiple linear regression, we usually are interested in\n",
    "answering a few important questions.\n",
    "     - Is at least one of the predictors X1,X2, . . . , Xp useful in predictingthe response?\n",
    "     - Do all the predictors help to explain Y , or is only a subset of the predictors useful? \n",
    "     - How well does the model fit the data?\n",
    "     - Given a set of predictor values, what\n",
    "- Using F-Stats\n",
    "<img src=\"Intro_Stats_2.png\"style=\"width: 400px;\"/>\n",
    "- Hence, when there is no relationship between the response and predictors,\n",
    "one would expect the F-statistic to take on a value close to 1.\n",
    "- Given these individual p-values for each variable, why do we need to look\n",
    "at the overall F-statistic? After all, it seems likely that if any one of the\n",
    "p-values for the individual variables is very small, then at least one of the\n",
    "predictors is related to the response. However, this logic is flawed, especially\n",
    "when the number of predictors p is large.\n",
    "- However, the F-statistic\n",
    "does not suffer from this problem because it adjusts for the number of\n",
    "predictors. Hence, if H0 is true, there is only a 5% chance that the Fstatistic\n",
    "will result in a p-value below 0.05, regardless of the number of\n",
    "predictors or the number of observations.\n",
    "- As discussed in the previous section, the first step in a multiple regression\n",
    "analysis is to compute the F-statistic and to examine the associated pvalue.\n",
    "- Mallow’s Cp, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted R2 (Will be discussed later)\n",
    "- How to select which variables, too much features make it impossible to look at all the values\n",
    "    - **Forward Selection**: We begin with the null model—a model that contains an intercept but no predictors. We then fit p simple linear regressions\n",
    "and add to the null model the variable that results in the\n",
    "lowest RSS. We then add to that model the variable that results in the lower RSS for the new two-variable model.\n",
    "        - Essentially we are adding features until we find a variable that doesn't add any more information\n",
    "    - **Backward selection**: We start with all variables in the model, and remove the variable with the largest p-value—that is, the variable that is the least statistically significant. The new (p − 1)-variable\n",
    "model is fit, and the variable with the largest p-value is removed. This\n",
    "procedure continues until a stopping rule is reached. For instance, we\n",
    "may stop when all remaining variables have a p-value below some\n",
    "threshold. \n",
    "        - Here, we add all the values and keep removing until we find we are removing an important variable\n",
    "    - Mixed Selection: This is a combination of forward and backward selection. We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. We continue\n",
    "to add variables one-by-one. Of course, as we noted with the\n",
    "Advertising example, the p-values for variables can become larger as\n",
    "new predictors are added to the model. Hence, if at any point the\n",
    "p-value for one of the variables in the model rises above a certain\n",
    "threshold, then we remove that variable from the model. We continue\n",
    "to perform these forward and backward steps until all variables\n",
    "in the model have a sufficiently low p-value, and all variables outside\n",
    "the model would have a large p-value if added to the model.\n",
    "- Making Prediction is difficult because we have to first take into we are estimating the parameters, thus it will never be accurate. Second, we are predicting the sample from the popualtion, so the popualtion is prob. a bit different. Third, we know that there are errors just because of randomizes and we will never be able take to take into account. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.3.2 Extensions of the Linear Model **\n",
    "- The additive assumption means that the effect of changes in a predictor Xj on the response Y is\n",
    "independent of the values of the other predictors. The linear assumption\n",
    "states that the change in the response Y due to a one-unit change in Xj is\n",
    "constant, regardless of the value of Xjthat if we look at the variables in the linear equation, they are not quite indepdent. In fact, one variable might increase the slope of the other\n",
    "- In marketing, this is known as a synergy effect,\n",
    "and in statistics it is referred to as an interaction effect. This means that there's an interaction effect between the variables.\n",
    "- The hierarchical\n",
    "principle states that if we include an interaction in a model, we\n",
    "hierarchical\n",
    "should also include the main effects, even if the p-values associated with principle\n",
    "their coefficients are not significant. In other words, if the interaction between\n",
    "X1 and X2 seems important, then we should include both X1 and\n",
    "X2 in the model even if their coefficient estimates have large p-values. The\n",
    "rationale for this principle is that if X1 × X2 is related to the response,\n",
    "then whether or not the coefficients of X1 or X2 are exactly zero is of little\n",
    "interest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.3.3 Potential Problems **\n",
    "1. Non-linearity of the Data\n",
    "    - To check for non-linearity, we can plot the residuals against the predictor. Ideally, the residual plot will show no fitted discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.\n",
    "2. Correlation of Error Terms\n",
    "    - An important assumption of the linear regression model is that the error\n",
    "terms are uncorrelated. What does this mean? For instance,\n",
    "if the errors are uncorrelated, then the fact that error term is positive provides\n",
    "little or no information about the sign of any other error term.\n",
    "- Why might correlations among the error terms occur? Such correlations\n",
    "frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time.\n",
    "3. Non-constant Variance of Error Terms\n",
    "- Another important assumption of the linear regression model is that the\n",
    "error terms have a constant variance\n",
    "- magnitude of the residuals tends to increase with the fitted\n",
    "values.\n",
    "- An example is shown in the left-hand panel of Figure 3.11, sticity\n",
    "in which the magnitude of the residuals tends to increase with the fitted\n",
    "values.\n",
    "5. High Leverage Points\n",
    "- We just saw that outliers are observations for which the response yi is\n",
    "unusual given the predictor xi. In contrast, observations with high leverage have an unusual value for xi.\n",
    "6. Collinearity\n",
    "- Collinearity refers to the situation in which two or more predictor variables are closely related to one another.\n",
    "- A simple way to detect collinearity is to look at the correlation matrix\n",
    "of the predictors.\n",
    "- Multicollinearity - Unfortunately, not all collinearity problems can be\n",
    "detected by inspection of the correlation matrix: it is possible for collinearity\n",
    "to exist between three or more variables even if no pair of variables\n",
    "has a particularly high correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Programming in R **\n",
    "- Check code in R\n",
    "- Define:\n",
    "    - Anova: Comparing the sample mean "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
