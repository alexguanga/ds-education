{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Notation **:\n",
    "- We will use n to represent the number of distinct data points, or observations,\n",
    "in our sample.\n",
    "- We will let p denote the number of variables that are\n",
    "available for use in making predictions.\n",
    "- For example, the Wage data set consists\n",
    "of 12 variables for 3,000 people, so we have n = 3,000 observations and\n",
    "p = 12 variables (such as year, age, wage, and more). Note that throughout\n",
    "this book, we indicate variable names using colored font: Variable Name.\n",
    "- T he samples or observations (from 1 to n) and\n",
    "j will be used to index the variables (from 1 to p).\n",
    "- In general, we will let xij represent the value of the jth variable for the\n",
    "ith observation, where i = 1, 2, . . ., n and j = 1, 2, . . . , p.\n",
    "- (Vectors are by default represented as columns.)\n",
    "- In this text, a vector of length n will always be denoted in lower case\n",
    "bold\n",
    "- However, vectors that are not of length n (such as feature vectors of length\n",
    "p, as in (1.1)) will be denoted in lower case normal font, e.g. a.\n",
    "- Scalars will\n",
    "also be denoted in lower case normal font, e.g. a.\n",
    "- We use yi to denote the ith observation of the variable on which we\n",
    "wish to make predictions, such as wage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is because Y is also a function of\n",
    "e, which, by definition, cannot be predicted using X. Therefore, variability\n",
    "associated with e also affects the accuracy of our predictions. This is known\n",
    "as the irreducible error, because no matter how well we estimate f, we\n",
    "cannot reduce the error introduced by e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.1.3 The Trade-Off Between Prediction Accuracy and Model\n",
    "Interpretability **\n",
    "- If we are mainly interested in inference, then restrictive models are much\n",
    "more interpretable. For instance, when inference is the goal, the linear\n",
    "model may be a good choice since it will be quite easy to understand\n",
    "the relationship between Y and X1,X2, . . . , Xp.\n",
    "- In contrast, very flexible\n",
    "approaches, such as the splines discussed in Chapter 7 and displayed in\n",
    "Figures 2.5 and 2.6, and the boosting methods discussed in Chapter 8, can\n",
    "lead to such complicated estimates of f that it is difficult to understand\n",
    "how any individual predictor is associated with the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.1.4 Supervised Versus Unsupervised Learning **\n",
    "- Many classical statistical learning\n",
    "methods such as linear regression and logistic regression (Chapter 4), as\n",
    "logistic\n",
    "well as more modern approaches such as GAM, boosting, and support vec- regression\n",
    "tor machines, operate in the supervised learning\n",
    "- In this setting, we are in some sense working blind; the situation\n",
    "is referred to as unsupervised because we lack a response variable\n",
    "that can supervise our analysis.\n",
    "- One statistical learning tool that we may use\n",
    "in this setting is cluster analysis, or clustering. The goal of cluster analysis\n",
    "cluster\n",
    "is to ascertain, on the basis of x1, . . . , xn, whether the observations fall into analysis\n",
    "relatively distinct groups. For example, in a market segmentation study we\n",
    "might observe multiple characteristics (variables) for potential customers,\n",
    "such as zip code, family income, and shopping habits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.1.5 Regression Versus Classification Problems**\n",
    "- Variables can be characterized as either quantitative or qualitative (also known as categorical)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.2 Assessing Model Accuracy** \n",
    "- There\n",
    "is no free lunch in statistics: no one method dominates all others over all\n",
    "possible data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.2.1 Measuring the Quality of Fit **\n",
    "- In the regression setting, the\n",
    "most commonly-used measure is the mean squared error (MSE)\n",
    "- The MSE in (2.5) is computed using the training data that was used to\n",
    "fit the model, and so should more accurately be referred to as the training\n",
    "MSE. But in general, we do not really care how well the method works\n",
    "training\n",
    "on the training data. Rather, we are interested in the accuracy of the pre- MSE\n",
    "dictions that we obtain when we apply our method to previously unseen\n",
    "test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.2.2 The Bias-Variance Trade-Off **\n",
    "- in order to minimize the expected test error,\n",
    "we need to select a statistical learning method that simultaneously achieves\n",
    "low variance and low bias.\n",
    "- Variance refers to the amount by which ˆ f would change if we\n",
    "estimated it using a different training data set. Since the training data\n",
    "are used to fit the statistical learning method, different training data sets\n",
    "will result in a different ˆ f. But ideally the estimate for f should not vary\n",
    "too much between training sets. However, if a method has high variance\n",
    "then small changes in the training data can result in large changes in ˆ f. In\n",
    "general, more flexible statistical methods have higher variance.\n",
    "- bias refers to the error that is introduced by approximating\n",
    "a real-life problem, which may be extremely complicated, by a much\n",
    "simpler model.\n",
    "- This is referred to as a trade-off because it is easy to obtain a method with extremely low bias but\n",
    "high variance (for instance, by drawing a curve that passes through every\n",
    "single training observation) or a method with very low variance but high\n",
    "bias (by fitting a horizontal line to the data). The challenge lies in finding\n",
    "a method for which both the variance and the squared bias are low. This\n",
    "trade-off is one of the most important recurring themes in this book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
